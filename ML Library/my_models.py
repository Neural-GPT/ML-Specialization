# -*- coding: utf-8 -*-
"""my_models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YlZ9Ng6L4v6Vuh-tikQAtlMXAKUa8HbA
"""

import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt

class Logistic_Regression:
    def __init__(self, epochs=501, alpha=0.01, lambda_=0):
        self.epochs = epochs
        self.alpha = alpha
        self.lambda_ = lambda_

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def compute_cost(self, x, y, w, b):
        m = x.shape[1]
        z = np.dot(w, x) + b
        g = self.sigmoid(z)
        epsilon = 1e-15  #to avoid log(0)
        g = np.clip(g, epsilon, 1 - epsilon)
        cost = (-1/m) * (np.dot(y, np.log(g.T)) + np.dot((1 - y), np.log(1 - g.T)))
        cost += (self.lambda_ / (2 * m)) * np.sum(w**2)
        return cost.item()

    def compute_gradients(self, x, y, w, b):
        m = x.shape[1]
        z = np.dot(w, x) + b
        g = self.sigmoid(z)
        dj_dw = (1/m) * np.dot((g - y), x.T) + (self.lambda_ / m) * w
        dj_db = (1/m) * np.sum(g - y)
        return dj_dw, dj_db

    def fit(self, x, y):
        if isinstance(x, pd.DataFrame): x = x.values
        if isinstance(y, pd.Series): y = y.values

        m, n = x.shape
        x = x.T
        y = y.reshape(1, m)

        w = np.zeros((1, x.shape[0]))
        b = 0

        self.w_history = []
        self.cost_history = []

        for i in range(self.epochs):
            cost = self.compute_cost(x, y, w, b)
            dw, db = self.compute_gradients(x, y, w, b)

            w -= self.alpha * dw
            b -= self.alpha * db

            self.w_history.append(w.copy())
            self.cost_history.append(cost)

            if i % 100 == 0 or i == self.epochs - 1:
                time.sleep(0.1)
                print(f"\033[1mEpoch {i}: cost = {cost:.4f}\033[0m") #for bold output

        self.coefficients = w
        self.bias = b

    def predict(self, x, threshold=0.5):
        if isinstance(x, pd.DataFrame): x = x.values
        x = x.T
        z = np.dot(self.coefficients, x) + self.bias
        probs = self.sigmoid(z)
        return (probs >= threshold).astype(int)

    def predict_proba(self, x):
        if isinstance(x, pd.DataFrame): x = x.values
        x = x.T
        z = np.dot(self.coefficients, x) + self.bias
        return self.sigmoid(z)

    def plot_cost(self):
        if not hasattr(self, 'cost_history') or not self.cost_history:
            raise ValueError("Model is not trained yet or no cost history available.")

        plt.figure(figsize=(8, 5))
        plt.plot(range(len(self.cost_history)), self.cost_history, label='Cost')
        plt.xlabel("Epochs")
        plt.ylabel("Cost")
        plt.title("Cost vs Epochs")
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.show()

    def plot_cost_log(self):
        if not hasattr(self, 'cost_history') or not self.cost_history:
            raise ValueError("Model is not trained yet or no cost history available.")

        plt.figure(figsize=(8, 5))
        plt.semilogy(range(len(self.cost_history)), self.cost_history, label='Cost (log scale)')
        plt.xlabel("Epochs")
        plt.ylabel("Cost (log scale)")
        plt.title("Cost vs Epochs (Logarithmic Scale)")
        plt.grid(True, which="both", ls="--")
        plt.legend()
        plt.tight_layout()
        plt.show()

class Linear_Regression:
    def __init__(self, epochs=501, alpha=0.01, lambda_=0):
        self.epochs = epochs
        self.alpha = alpha
        self.lambda_ = lambda_

    def compute_cost(self, x, y, w, b):
        m = x.shape[1]
        z = np.dot(w, x) + b
        cost = (1 / m) * np.sum((z - y)**2)
        if self.lambda_ > 0:
            cost += (self.lambda_ / (2 * m)) * np.sum(w**2)
        return cost.item()  # ensures clean float printing

    def compute_gradients(self, x, y, w, b):
        m = x.shape[1]
        z = np.dot(w, x) + b
        dj_dw = (1/m) * np.dot((z - y), x.T) + (self.lambda_ / m) * w
        dj_db = (1/m) * np.sum(z - y)
        return dj_dw, dj_db

    def fit(self, x, y):
        if isinstance(x, pd.DataFrame): x = x.values
        if isinstance(y, pd.Series): y = y.values
        m, n = x.shape
        x = x.T
        y = y.reshape(1, m)

        w = np.zeros((1, x.shape[0]))
        b = 0

        self.w_history = []
        self.cost_history = []

        for i in range(self.epochs):
            cost = self.compute_cost(x, y, w, b)
            dw, db = self.compute_gradients(x, y, w, b)

            w -= self.alpha * dw
            b -= self.alpha * db

            self.w_history.append(w.copy())
            self.cost_history.append(cost)

            if i % 100 == 0 or i == self.epochs - 1:
                time.sleep(0.1)
                print(f"\033[1mEpoch {i}: cost = {cost:.4f}\033[0m")

        self.coefficients = w
        self.bias = b

    def predict(self, x):
        if isinstance(x, pd.DataFrame): x = x.values
        x = x.T
        z = np.dot(self.coefficients, x) + self.bias
        return z


    def plot_cost(self):
        if not hasattr(self, 'cost_history') or not self.cost_history:
            raise ValueError("Model is not trained yet or no cost history available.")

        plt.figure(figsize=(8, 5))
        plt.plot(range(len(self.cost_history)), self.cost_history, label='Cost')
        plt.xlabel("Epochs")
        plt.ylabel("Cost")
        plt.title("Cost vs Epochs")
        plt.grid(True)
        plt.legend()
        plt.tight_layout()
        plt.show()

    def plot_cost_log(self):
        if not hasattr(self, 'cost_history') or not self.cost_history:
            raise ValueError("Model is not trained yet or no cost history available.")

        plt.figure(figsize=(8, 5))
        plt.semilogy(range(len(self.cost_history)), self.cost_history, label='Cost (log scale)')
        plt.xlabel("Epochs")
        plt.ylabel("Cost (log scale)")
        plt.title("Cost vs Epochs (Logarithmic Scale)")
        plt.grid(True, which="both", ls="--")
        plt.legend()
        plt.tight_layout()
        plt.show()